# -*- coding: utf-8 -*-
"""Classification on Imbalanced Data using Python.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YC_1CVyiHL95gm1arckv59g4muF4On0b
"""

import pandas as pd

# load the dataset
data = pd.read_csv("Insurance claims data.csv")

print(data.head())

data.info()

data.isnull().sum()

import matplotlib.pyplot as plt

# Calculate the counts for each claim status
claim_status_counts = data['claim_status'].value_counts()

# Define labels and colors for the pie chart
labels = claim_status_counts.index
colors = ['#FFA07A', '#FA8072', '#E9967A', '#F08080', '#CD5C5C']  # Different shades of orange and red

# Create the pie chart
plt.figure(figsize=(6, 6))
plt.pie(claim_status_counts, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
plt.title('Distribution of Claim Status')
plt.show()

# Selecting numerical columns for analysis
numerical_columns = ['subscription_length', 'vehicle_age', 'customer_age']

# Defining custom colors for each plot
colors = ['#FF9999', '#66B3FF', '#99FF99']  # Light red, light blue, light green

# Plotting distributions of numerical features with custom colors
plt.figure(figsize=(15, 5))
for i, (column, color) in enumerate(zip(numerical_columns, colors), 1):
    plt.subplot(1, 3, i)
    sns.histplot(data[column], bins=30, kde=True, color=color)
    plt.title(f'Distribution of {column}')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# selecting some relevant categorical columns for analysis
categorical_columns = ['region_code', 'segment', 'fuel_type']

# Define a custom color palette with distinct colors
custom_palette = sns.color_palette("husl", len(data['region_code'].unique()))
# or use any other palette like 'Set3', 'Paired', etc.

# plotting distributions of categorical features using bar plots with custom colors
plt.figure(figsize=(15, 10))
for i, column in enumerate(categorical_columns, 1):
    plt.subplot(3, 1, i)

    # Calculate category proportions
    category_counts = data[column].value_counts(normalize=True)

    # Create a bar plot
    sns.barplot(x=category_counts.index, y=category_counts.values, palette=custom_palette)

    plt.title(f'Distribution of {column}', fontsize=14)
    plt.xlabel(column, fontsize=12)
    plt.ylabel('Proportion', fontsize=12)
    plt.xticks(rotation=45, ha='right', fontsize=10)  # Rotate x-axis labels for better readability
    plt.yticks(fontsize=10)

plt.tight_layout()
plt.show()

from sklearn.utils import resample

# separate majority and minority classes
majority = data[data.claim_status == 0]
minority = data[data.claim_status == 1]

# oversample the minority class
minority_oversampled = resample(minority,
                                replace=True,
                                n_samples=len(majority),
                                random_state=42)

# combine majority class with oversampled minority class
oversampled_data = pd.concat([majority, minority_oversampled])

# check the distribution of undersampled and oversampled datasets
oversampled_distribution = oversampled_data.claim_status.value_counts()

oversampled_distribution

import matplotlib.pyplot as plt
import seaborn as sns

# plotting the distribution of numerical features with respect to 'claim_status' using kdeplots
plt.figure(figsize=(15, 5))

# 'customer_age' distribution
plt.subplot(1, 3, 1)
sns.kdeplot(data=oversampled_data, x='customer_age', hue='claim_status', fill=True, palette=['#007bff', '#dc3545'])
plt.title('Customer Age Distribution by Claim Status')

# 'vehicle_age' distribution
plt.subplot(1, 3, 2)
sns.kdeplot(data=oversampled_data, x='vehicle_age', hue='claim_status', fill=True, palette=['#007bff', '#dc3545'])
plt.title('Vehicle Age Distribution by Claim Status')

# 'subscription_length' distribution
plt.subplot(1, 3, 3)
sns.kdeplot(data=oversampled_data, x='subscription_length', hue='claim_status', fill=True, palette=['#007bff', '#dc3545'])
plt.title('Subscription Length Distribution by Claim Status')

plt.tight_layout()
plt.show()

from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder

# encode categorical variables
le = LabelEncoder()
encoded_data = data.apply(lambda col: le.fit_transform(col) if col.dtype == 'object' else col)

# separate features and target variable
X = encoded_data.drop('claim_status', axis=1)
y = encoded_data['claim_status']

# create a random forest classifier model
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X, y)

# get feature importance
feature_importance = rf_model.feature_importances_

# create a dataframe for visualization of feature importance
features_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importance})
features_df = features_df.sort_values(by='Importance', ascending=False)

print(features_df.head(10))  # displaying the top 10 important features}

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from sklearn.ensemble import RandomForestClassifier

# drop 'Policy_id' column from the data
oversampled_data = oversampled_data.drop('policy_id', axis=1)

# prepare the oversampled data
X_oversampled = oversampled_data.drop('claim_status', axis=1)
y_oversampled = oversampled_data['claim_status']

# encoding categorical columns
X_oversampled_encoded = X_oversampled.apply(lambda col: LabelEncoder().fit_transform(col) if col.dtype == 'object' else col)

# splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X_oversampled_encoded, y_oversampled, test_size=0.3, random_state=42)

# create and train the Random Forest model
rf_model_oversampled = RandomForestClassifier(random_state=42)
rf_model_oversampled.fit(X_train, y_train)

# predictions
y_pred = rf_model_oversampled.predict(X_test)

print(classification_report(y_test, y_pred))

original_encoded = data.drop('policy_id', axis=1).copy()
encoders = {col: LabelEncoder().fit(X_oversampled[col]) for col in X_oversampled.select_dtypes(include=['object']).columns}

for col in original_encoded.select_dtypes(include=['object']).columns:
    if col in encoders:
        original_encoded[col] = encoders[col].transform(original_encoded[col])

original_encoded_predictions = rf_model_oversampled.predict(original_encoded.drop('claim_status', axis=1))

comparison_df = pd.DataFrame({
    'Actual': original_encoded['claim_status'],
    'Predicted': original_encoded_predictions
})

print(comparison_df.head(10))

import matplotlib.pyplot as plt
import numpy as np

correctly_classified = (comparison_df['Actual'] == comparison_df['Predicted']).sum()
incorrectly_classified = (comparison_df['Actual'] != comparison_df['Predicted']).sum()

classification_counts = [correctly_classified, incorrectly_classified]
labels = ['Correctly Classified', 'Misclassified']

# Create a horizontal bar chart
plt.figure(figsize=(8, 4))
plt.barh(labels, classification_counts, color=['#66b3ff', '#ff9999'])
plt.xlabel('Count', fontsize=12)
plt.ylabel('Classification', fontsize=12)
plt.title('Classification Accuracy', fontsize=16)

# Add count labels to the bars
for i, count in enumerate(classification_counts):
    plt.text(count, i, str(count), ha='right', va='center', fontsize=12, color='white')

plt.show()

